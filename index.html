<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Juze Zhang</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Juze Zhang
                </p>
                <p>I'm currently a postdoc at <a href="https://stai.stanford.edu/">Stanford Translational AI Lab (STAI)</a> and <a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a> hosted by <a href="https://stanford.edu/~eadeli/">Prof. Ehsan Adeli</a>. I just received my Ph.D. from the University of the Chinese Academy of Sciences (ShanghaiTech).
                </p>
                <p>
                  I am passionate about human-centric motion perception, interaction, and generation, with a focus on developing foundational agents capable of perceiving, understanding, and interacting with the 3D world.
                </p>
                <p style="text-align:center">
                  <a href="mailto:juze@stanford.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com.hk/citations?user=qbovRUIAAAAJ&hl=zh-CN">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/JuzeZhang_">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Juzezhang">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/Headshot-Juze.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 0;" alt="profile photo" src="images/Headshot-Juze.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
<!--                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.-->
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!--###### Language of Motion ####-->
          <tr>
            <td style="padding:16px; width:30%; vertical-align:middle; text-align:center;">
              <img src="images/lom.png" style="width:100%; max-width:160px; height:auto; display:block; margin:auto;">
            </td>
            <td style="padding:16px; width:70%; vertical-align:middle;">
              <a href="https://languageofmotion.github.io/">
                <span class="papertitle">The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion</span>
              </a>
              <br>
              <a href="https://changan.io/">Changan Chen*</a>, <strong>Juze Zhang*</strong>,
              <a href="https://scholar.google.com.hk/citations?user=UjpM6IAAAAAJ&hl=zh-CN&oi=ao">Shrinidhi Kowshika Lakshmikanth*</a>,
              <a href="#">Yusu Fang</a>, <a href="https://dsaurus.github.io/saurus/">Ruizhi Shao</a>,
              <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
              <a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>,
              <a href="https://stanford.edu/~eadeli/">Ehsan Adeli</a>,
              <br>
              <em>arXiv</em>, 2024
              <br>
              <a href="https://languageofmotion.github.io/">project page</a> /
              <a href="https://arxiv.org/pdf/2412.10523">arXiv</a>
            </td>
          </tr>


          <!--###### HOIM3 ####-->

          <tr>
            <td style="padding:16px; width:30%; vertical-align:middle; text-align:center;">
              <img src="images/hoim3.gif" style="width:100%; max-width:160px; height:auto; display:block; margin:auto;">
            </td>
            <td style="padding:16px; width:70%; vertical-align:middle;">
              <a href="https://juzezhang.github.io/HOIM3_ProjectPage/">
                <span class="papertitle">HOI-M3: Capture Multiple Humans and Objects Interaction within Contextual Environment</span>
              </a>
              <br>
              <strong>Juze Zhang*</strong>,
              <a href="https://zhanglele12138.github.io/">Jingyan Zhang*</a>,
              <a href="">Zining Song</a>,
              <a href="https://bio.zhanheshi.com/">Zhanhe Shi</a>,
              <a href="https://afterjourney00.github.io/">Chengfeng Zhao</a>,
              <a href="https://shiye21.github.io/">Ye Shi</a>,
              <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
              <a href="https://www.xu-lan.com/">Lan Xu</a>,
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
              <br>
              <em>CVPR</em>, 2024, Highlight
              <br>
              <a href="https://languageofmotion.github.io/">project page</a> /
              <a href="https://arxiv.org/pdf/2412.10523">arXiv</a> /
              <a href="https://github.com/Juzezhang/NeuralDome_Toolbox">code</a> /
              <a href="https://drive.google.com/drive/u/1/folders/1bT7J0XnbUx5goixgJRWJxpycOFffpwOc">dataset</a>
            </td>
          </tr>

          <!--###### I'm HOI ####-->
          <tr>
            <td style="padding:16px; width:30%; vertical-align:middle; text-align:center;">
              <img src="images/imhoi.gif" style="width:100%; max-width:160px; height:auto; display:block; margin:auto;">
            </td>
            <td style="padding:16px; width:70%; vertical-align:middle;">
              <a href="https://afterjourney00.github.io/IM-HOI.github.io/">
                <span class="papertitle">Iâ€™M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions</span>
              </a>
              <br>
              <a href="https://afterjourney00.github.io/">Chengfeng Zhao</a>,
              <strong>Juze Zhang</strong>,
              <a href="https://alt-js.github.io/">Jiashen Du</a>,
              <a href="https://cunkaixin.netlify.app/">Ziwei Shan</a>,
              <a href="">Junye Wang</a>,
              <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
              <a href="https://www.xu-lan.com/">Lan Xu</a>,
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://afterjourney00.github.io/IM-HOI.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2312.08869">arXiv</a> /
              <a href="https://github.com/AfterJourney00/IMHD-Dataset">code</a> /
              <a href="https://docs.google.com/forms/d/e/1FAIpQLScFRTK4zO0LQSgz_vgo-mowPiDnLlaYhbEjfBB2iUvB815i7Q/viewform">dataset</a>
            </td>
          </tr>



          <!--###### Both2hand ####-->
          <tr>
            <td style="padding:16px; width:30%; vertical-align:middle; text-align:center;">
              <img src="images/both2hand.gif" style="width:100%; max-width:160px; height:auto; display:block; margin:auto;">
            </td>
            <td style="padding:16px; width:70%; vertical-align:middle;">
              <a href="https://godheritage.github.io/BOTH2Hands/">
                <span class="papertitle">BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics</span>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=glTuEzEAAAAJ">Wenqian Zhang</a>,
              <a href="">Molin Huang</a>,
              <a href="">Yuxuan Zhou</a>,
              <strong>Juze Zhang</strong>,
              <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
              <a href="https://www.xu-lan.com/">Lan Xu</a>,
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://godheritage.github.io/BOTH2Hands/">project page</a> /
              <a href="https://arxiv.org/abs/2312.07937">arXiv</a> /
              <a href="https://github.com/Godheritage/BOTH2Hands">code</a> /
              <a href="https://github.com/Godheritage/BOTH2Hands">dataset</a>
            </td>
          </tr>

          <!--###### Neuraldome ####-->
          <tr>
            <td style="padding:16px; width:30%; vertical-align:middle; text-align:center;">
              <img src="images/neuraldome.gif" style="width:100%; max-width:160px; height:auto; display:block; margin:auto;">
            </td>
            <td style="padding:16px; width:70%; vertical-align:middle;">
              <a href="https://juzezhang.github.io/NeuralDome/">
                <span class="papertitle">NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions</span>
              </a>
              <br>
              <strong>Juze Zhang</strong>,
              <a href="https://haiminluo.github.io/">Haimin Luo</a>,
              <a href="">Hongdi Yang</a>,
              <a href="">Xinru Xu</a>,
              <a href="">Qianyang Wu</a>,
              <a href="https://shiye21.github.io/">Ye Shi</a>,
              <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
              <a href="https://www.xu-lan.com/">Lan Xu</a>,
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://juzezhang.github.io/NeuralDome/">project page</a> /
              <a href="https://arxiv.org/pdf/2212.07626">arXiv</a> /
              <a href="https://github.com/Juzezhang/NeuralDome_Toolbox">code</a> /
              <a href="https://drive.google.com/drive/u/1/folders/1-QHvcwa71Wk7rdfnQrOyInqK-SWK6lRA">dataset</a>
            </td>
          </tr>


          <!--###### IKOL ####-->
          <tr>
            <td style="padding:16px; width:30%; vertical-align:middle; text-align:center;">
              <img src="images/IKOL.png" style="width:100%; max-width:160px; height:auto; display:block; margin:auto;">
            </td>
            <td style="padding:16px; width:70%; vertical-align:middle;">
              <a href="https://juzezhang.github.io/IKOL-webpage/">
                <span class="papertitle">IKOL: Inverse kinematics optimization layer for 3D human pose and shape estimation via Gauss-Newton differentiation</span>
              </a>
              <br>
              <strong>Juze Zhang</strong>,
              <a href="https://shiye21.github.io/">Ye Shi</a>,
              <a href="https://yuexinma.me/">Yuexin Ma</a>,
              <a href="https://www.xu-lan.com/">Lan Xu</a>,
              <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
              <br>
              <em>AAAI</em>, 2023, oral
              <br>
              <a href="https://juzezhang.github.io/IKOL-webpage/">project page</a> /
              <a href="https://arxiv.org/pdf/2302.01058v2">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=RvaC7w-rIhg">video</a> /
              <a href="https://github.com/Juzezhang/IKOL">code</a> /
            </td>
          </tr>

          <!--###### fusionpose ####-->
          <tr>
            <td style="padding:16px; width:30%; vertical-align:middle; text-align:center;">
              <img src="images/fusionpose.jpeg" style="width:100%; max-width:160px; height:auto; display:block; margin:auto;">
            </td>
            <td style="padding:16px; width:70%; vertical-align:middle;">
              <a href="https://arxiv.org/pdf/2211.16951">
                <span class="papertitle">Weakly Supervised 3D Multi-person Pose Estimation for Large-scale Scenes based on Monocular Camera and Single LiDAR</span>
              </a>
              <br>
              <a href="https://coralemon.github.io/">Peishan Cong</a>,
              <a href="https://scholar.google.com/citations?user=29Gs2nQAAAAJ&hl=zh-CN">Yiteng Xu</a>,
              <a href="https://ren-ym.github.io/">Yiming Ren</a>,
              <strong>Juze Zhang</strong>,
              <a href="https://www.xu-lan.com/">Lan Xu</a>,
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
              <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
              <a href="https://yuexinma.me/">Yuexin Ma</a>,
              <br>
              <em>AAAI</em>, 2023, oral
              <br>
              <a href="">project page</a> /
              <a href="https://arxiv.org/pdf/2211.16951">arXiv</a> /
            </td>
          </tr>


          <!--###### MAR ####-->
          <tr>
            <td style="padding:16px; width:30%; vertical-align:middle; text-align:center;">
              <img src="images/mar.jpeg" style="width:100%; max-width:160px; height:auto; display:block; margin:auto;">
            </td>
            <td style="padding:16px; width:70%; vertical-align:middle;">
              <a href="https://arxiv.org/pdf/2207.07900">
                <span class="papertitle">Mutual Adaptive Reasoning for Monocular 3D Multi-Person Pose Estimations</span>
              </a>
              <br>
              <strong>Juze Zhang</strong>,
              <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a>,
              <a href="https://shiye21.github.io/">Ye Shi</a>,
              <a href="http://www.hislab.cn/">Fei Gao</a>,
              <a href="https://www.xu-lan.com/">Lan Xu</a>,
              <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
              <br>
              <em>ACMMM</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2211.16951">arXiv</a> /
            </td>
          </tr>

        </td>
      </tr>
    </table>
  </body>
</html>
