---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a PHD student at Chinese Academy of Sciences (ShanghaiTech) Visual & Data Intelligence Center(VDI Center), supervised by [Prof. Jingya Wang](https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/). I am also fortunate to work closely with [Prof. Jingyi Yu](http://www.yu-jingyi.com/) and [Prof. Lan Xu](https://www.xu-lan.com/). Also, I serve as the CEO co-founder of ElanTech Co., Ltd.

My research interests include:

- 3D computer vision/graphics,
- Human-centric motion perception, interaction, and generation,
- LLM-enhanced motion reasoning and analysis.

# üî• News
- *2024.02*: &nbsp;üéâüéâ Three paper accepted to CVPR 2024. 
- *2023.03*: &nbsp;üéâüéâ One paper accepted to CVPR 2023. 
- *2022.11*: &nbsp;üéâüéâ Two paper accepted to AAAI 2023. 
- *2022.06*: &nbsp;üéâüéâ One paper accepted to ACMMM 2022. 

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/hoim3.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

HOI-M3: Capture Multiple Humans and Objects Interaction within Contextual
Environment

**Juze Zhang**, Jingyan Zhang, Zining Song, Zhanhe Shi, Chengfeng Zhao, Jingyi Yu, Lan Xu, Jingya Wang


</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/imhoi.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions](https://arxiv.org/pdf/2312.08869.pdf)

Chengfeng Zhao, **Juze Zhang**, Jiashen Du, Ziwei Shan, Junye Wang, Jingyi Yu, Jingya Wang, Lan Xu

[[**Project**](https://afterjourney00.github.io/IM-HOI.github.io/)] [[**Code**](https://github.com/AfterJourney00/IMHD-Dataset)] [[**Data**](https://github.com/AfterJourney00/IMHD-Dataset)] <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/both2hand.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics](https://arxiv.org/pdf/2312.07937.pdf)

Wenqian Zhang, Molin Huang, Yuxuan Zhou, **Juze Zhang**, Jingyi Yu, Jingya Wang, Lan Xu

[[**Project**]](https://godheritage.github.io/) [[**Code**](https://github.com/Godheritage/BOTH2Hands)] [[**Data**](https://github.com/Godheritage/BOTH2Hands)] <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/neuraldome.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NeuralDome_A_Neural_Modeling_Pipeline_on_Multi-View_Human-Object_Interactions_CVPR_2023_paper.pdf)

**Juze Zhang**, Haimin Luo, Hongdi Yang, Xinru Xu, Qianyang Wu, Ye Shi, Jingyi Yu, Lan Xu, Jingya Wang

[[**Project**]](https://juzezhang.github.io/NeuralDome/) [[**Video**]](https://www.youtube.com/watch?v=Nb82f5dm2GE) [[**Code**](https://github.com/Juzezhang/NeuralDome_Toolbox)] [[**Data**](https://drive.google.com/drive/folders/1-QHvcwa71Wk7rdfnQrOyInqK-SWK6lRA)] <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023 (oral)</div><img src='images/IKOL.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[IKOL: Inverse kinematics optimization layer for 3D human pose and shape estimation via Gauss-Newton differentiation](https://arxiv.org/pdf/2302.01058v2.pdf)

**Juze Zhang**, Ye Shi, Yuexin Ma, Lan Xu, Jingyi Yu, Jingya Wang

[[**Project**]](https://juzezhang.github.io/IKOL-webpage/) [[**Video**]](https://www.youtube.com/watch?v=RvaC7w-rIhg) [[**Code**](https://github.com/Juzezhang/IKOL)] <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023 (oral)</div><img src='images/fusionpose.jpeg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Weakly Supervised 3D Multi-person Pose Estimation for Large-scale Scenes based on Monocular Camera and Single LiDAR](https://arxiv.org/pdf/2211.16951.pdf)

Peishan Cong, Yiteng Xu, Yiming Ren, **Juze Zhang**, Lan Xu, Jingya Wang, Jingyi Yu, Yuexin Ma

[**Project**] [[**Code**](https://github.com/4DVLab/FusionPose)] <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACMMM 2022</div><img src='images/mar.jpeg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Mutual Adaptive Reasoning for Monocular 3D Multi-Person Pose Estimations](https://arxiv.org/pdf/2302.01058v2.pdf)

**Juze Zhang**, Jingya Wang, Ye Shi, Fei Gao, Lan Xu, Jingyi Yu

[**Project**] [**Video**] <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

</div>
</div>

[//]: # (# üéñ Honors and Awards)

[//]: # (- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. )

[//]: # (- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. )
[//]: # ()
[//]: # (# üìñ Educations)

[//]: # (- *2019.06 - 2022.04 &#40;now&#41;*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. )

[//]: # (- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. )

[//]: # ()
[//]: # (# üí¨ Invited Talks)

[//]: # (- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. )

[//]: # (- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]]&#40;https://github.com/&#41;)

[//]: # ()
[//]: # (# üíª Internships)

[//]: # (- *2019.05 - 2020.02*, [Lorem]&#40;https://github.com/&#41;, China.)